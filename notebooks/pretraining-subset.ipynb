{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cf48d2-ff83-43cb-ab3c-8f556c230b30",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7e94f8-2488-4f66-9dba-d9cc3040e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bc174-f440-4b22-83c6-0dd68d5f9274",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e252bd70-3b60-45b5-a21e-8e338a6986c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_n_save_indices(data_path, save_path, transform, splits=[.70, .15]):\n",
    "    \"\"\"\n",
    "    Function to split a torchvision dataset and save the indices of the splits\n",
    "    so that they can be reused.\n",
    "\n",
    "    ensures recreation of the splits\n",
    "    \"\"\"\n",
    "    dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    \n",
    "    train_size = int(splits[0] * len(dataset))\n",
    "    valid_size = int(splits[1] * len(dataset))\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "    torch.save({\n",
    "        'train_indices': train_dataset.indices, \n",
    "        'valid_indices': valid_dataset.indices, \n",
    "        'test_indices': test_dataset.indices\n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a44d6db-885f-43ec-a838-fdf94c99095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, model, criterion, optimizer, train_loader, valid_loader, device, save_dir, save_every, print_every):\n",
    "    hist_train_loss = []\n",
    "    hist_valid_loss = []\n",
    "    hist_train_accs = []\n",
    "    hist_valid_accs = []\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_corr = 0\n",
    "        valid_corr = 0\n",
    "        batch_corr = 0\n",
    "\n",
    "        for i, (X_train, y_train) in enumerate(train_loader):\n",
    "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "            train_pred = model(X_train)\n",
    "            train_loss = criterion(train_pred, y_train)\n",
    "\n",
    "            train_predicted = torch.max(train_pred.data, 1)[1]\n",
    "            batch_corr = (train_predicted == y_train).sum()\n",
    "            train_corr += batch_corr\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print progress\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print(f'Iteration {i + 1}/{len(train_loader)}; Loss: {train_loss.item()}')\n",
    "\n",
    "            # checkpoint saving based on iterations\n",
    "            if (i + 1) % save_every == 0:\n",
    "                checkpoint_path = os.path.join(save_dir, f'epoch-{epoch}_iter-{i + 1}_checkpoint.pth')\n",
    "                torch.save({\n",
    "                    'epoch': epoch, \n",
    "                    'iteration': i + 1, \n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict(), \n",
    "                    'loss': train_loss.item()\n",
    "                }, checkpoint_path)\n",
    "                print(f'Checkpoint saved at {checkpoint_path} after {i + 1} iterations of epoch {epoch}')\n",
    "\n",
    "        train_accuracy = train_corr.item() / len(train_loader.dataset)\n",
    "\n",
    "        hist_train_loss.append(train_loss.item())\n",
    "        hist_train_accs.append(train_accuracy)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_valid, y_valid in valid_loader:\n",
    "                X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "\n",
    "                valid_pred = model(X_valid)\n",
    "\n",
    "                valid_predicted = torch.max(valid_pred.data, 1)[1]\n",
    "                valid_corr += (valid_predicted == y_valid).sum()\n",
    "\n",
    "        valid_accuracy = valid_corr.item() / len(valid_loader.dataset)\n",
    "        valid_loss = criterion(valid_pred, y_valid)\n",
    "\n",
    "        hist_valid_loss.append(valid_loss.item())\n",
    "        hist_valid_accs.append(valid_accuracy)\n",
    "\n",
    "        print(\n",
    "            f'[epoch: {epoch}]\\n', \n",
    "            f'- train loss: {train_loss.item():}, train accuracy: {train_accuracy}\\n',\n",
    "            f'- valid loss: {valid_loss.item()}, valid accuracy: {valid_accuracy}'\n",
    "        )\n",
    "\n",
    "        if valid_accuracy > best_accuracy:\n",
    "            best_accuracy = valid_accuracy\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'hist_train_loss': hist_train_loss, \n",
    "                'hist_train_accs': hist_train_accs,\n",
    "                'hist_valid_loss': hist_valid_loss, \n",
    "                'hist_valid_accs': hist_valid_accs\n",
    "            }, os.path.join(save_dir, f'best_model.pth'))\n",
    "            print(f'- New best model saves with accuracy {valid_accuracy:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9664c80-14d5-4f8a-ba83-6fe15152de24",
   "metadata": {},
   "source": [
    "# Main Excecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c21cc42-afb8-4102-a128-a5ab8128da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.manual_seed(42) if device.type == 'cuda' else torch.manual_seed(42)\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea63e7a-a931-4120-a81f-575306f16166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING THE DATASET\n",
    "\n",
    "data_path = '../data/pretraining-data'\n",
    "save_path = '../data/pretraining-dataset-indices.pth'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d9f62-cfbd-4d67-b847-c4ebea546910",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_n_save_indices(data_path, save_path, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f319e-f9ef-4db2-ba0e-a49577d2d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indices\n",
    "indices = torch.load(save_path)\n",
    "\n",
    "train_indices = indices['train_indices']\n",
    "valid_indices = indices['valid_indices']\n",
    "test_indices = indices['test_indices']\n",
    "\n",
    "subset_percent = 0.2\n",
    "train_len = int(0.2 * len(train_indices))\n",
    "valid_len = int(0.2 * len(valid_indices))\n",
    "test_len = int(0.2 * len(test_indices))\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices[:train_len])\n",
    "valid_dataset = Subset(dataset, valid_indices[:valid_len])\n",
    "test_dataset = Subset(dataset, test_indices[:test_len])\n",
    "\n",
    "bs = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=bs, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc57a696-25ed-41aa-99e9-a12a81fd2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad5497/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20/540; Loss: 1.610893964767456\n",
      "Iteration 40/540; Loss: 1.4994583129882812\n",
      "Iteration 60/540; Loss: 1.2412899732589722\n",
      "Checkpoint saved at models-test/epoch-0_iter-60_checkpoint.pth after 60 iterations of epoch 0\n",
      "Iteration 80/540; Loss: 0.9037249684333801\n",
      "Iteration 100/540; Loss: 0.8036942481994629\n",
      "Iteration 120/540; Loss: 0.8705576062202454\n",
      "Checkpoint saved at models-test/epoch-0_iter-120_checkpoint.pth after 120 iterations of epoch 0\n",
      "Iteration 140/540; Loss: 0.7765486240386963\n",
      "Iteration 160/540; Loss: 0.7585942149162292\n",
      "Iteration 180/540; Loss: 0.8005860447883606\n",
      "Checkpoint saved at models-test/epoch-0_iter-180_checkpoint.pth after 180 iterations of epoch 0\n",
      "Iteration 200/540; Loss: 0.8790202140808105\n",
      "Iteration 220/540; Loss: 0.7823211550712585\n",
      "Iteration 240/540; Loss: 0.7159841060638428\n",
      "Checkpoint saved at models-test/epoch-0_iter-240_checkpoint.pth after 240 iterations of epoch 0\n",
      "Iteration 260/540; Loss: 0.7567704916000366\n",
      "Iteration 280/540; Loss: 0.6931222677230835\n",
      "Iteration 300/540; Loss: 0.7599588632583618\n",
      "Checkpoint saved at models-test/epoch-0_iter-300_checkpoint.pth after 300 iterations of epoch 0\n",
      "Iteration 320/540; Loss: 0.7781280279159546\n",
      "Iteration 340/540; Loss: 0.7371274828910828\n",
      "Iteration 360/540; Loss: 0.6569238901138306\n",
      "Checkpoint saved at models-test/epoch-0_iter-360_checkpoint.pth after 360 iterations of epoch 0\n",
      "Iteration 380/540; Loss: 0.680234968662262\n",
      "Iteration 400/540; Loss: 0.7587292194366455\n",
      "Iteration 420/540; Loss: 0.7229719161987305\n",
      "Checkpoint saved at models-test/epoch-0_iter-420_checkpoint.pth after 420 iterations of epoch 0\n",
      "Iteration 440/540; Loss: 0.7996165156364441\n",
      "Iteration 460/540; Loss: 0.687829315662384\n",
      "Iteration 480/540; Loss: 0.8704547882080078\n",
      "Checkpoint saved at models-test/epoch-0_iter-480_checkpoint.pth after 480 iterations of epoch 0\n",
      "Iteration 500/540; Loss: 0.7062006592750549\n",
      "Iteration 520/540; Loss: 0.7062832117080688\n",
      "Iteration 540/540; Loss: 0.7198731303215027\n",
      "Checkpoint saved at models-test/epoch-0_iter-540_checkpoint.pth after 540 iterations of epoch 0\n",
      "[epoch: 0]\n",
      " - train loss: 0.7198731303215027, train accuracy: 0.5633101851851852\n",
      " - valid loss: 0.7068927884101868, valid accuracy: 0.6134259259259259\n",
      "- New best model saves with accuracy 0.6134%\n",
      "Iteration 20/540; Loss: 0.6761704683303833\n",
      "Iteration 40/540; Loss: 0.6736990809440613\n",
      "Iteration 60/540; Loss: 0.6993700265884399\n",
      "Checkpoint saved at models-test/epoch-1_iter-60_checkpoint.pth after 60 iterations of epoch 1\n",
      "Iteration 80/540; Loss: 0.6506392955780029\n",
      "Iteration 100/540; Loss: 0.7169325351715088\n",
      "Iteration 120/540; Loss: 0.7097728252410889\n",
      "Checkpoint saved at models-test/epoch-1_iter-120_checkpoint.pth after 120 iterations of epoch 1\n",
      "Iteration 140/540; Loss: 0.5739110112190247\n",
      "Iteration 160/540; Loss: 0.7302427291870117\n",
      "Iteration 180/540; Loss: 0.6553246378898621\n",
      "Checkpoint saved at models-test/epoch-1_iter-180_checkpoint.pth after 180 iterations of epoch 1\n",
      "Iteration 200/540; Loss: 0.676527738571167\n",
      "Iteration 220/540; Loss: 0.6934009790420532\n",
      "Iteration 240/540; Loss: 0.6665887832641602\n",
      "Checkpoint saved at models-test/epoch-1_iter-240_checkpoint.pth after 240 iterations of epoch 1\n",
      "Iteration 260/540; Loss: 0.7326309680938721\n",
      "Iteration 280/540; Loss: 0.7009292840957642\n",
      "Iteration 300/540; Loss: 0.7105575203895569\n",
      "Checkpoint saved at models-test/epoch-1_iter-300_checkpoint.pth after 300 iterations of epoch 1\n",
      "Iteration 320/540; Loss: 0.76957768201828\n",
      "Iteration 340/540; Loss: 0.707994818687439\n",
      "Iteration 360/540; Loss: 0.8125250339508057\n",
      "Checkpoint saved at models-test/epoch-1_iter-360_checkpoint.pth after 360 iterations of epoch 1\n",
      "Iteration 380/540; Loss: 0.6284523010253906\n",
      "Iteration 400/540; Loss: 0.767585277557373\n",
      "Iteration 420/540; Loss: 0.674745500087738\n",
      "Checkpoint saved at models-test/epoch-1_iter-420_checkpoint.pth after 420 iterations of epoch 1\n",
      "Iteration 440/540; Loss: 0.6080477237701416\n",
      "Iteration 460/540; Loss: 0.6142777800559998\n",
      "Iteration 480/540; Loss: 0.6303535103797913\n",
      "Checkpoint saved at models-test/epoch-1_iter-480_checkpoint.pth after 480 iterations of epoch 1\n",
      "Iteration 500/540; Loss: 0.7321838736534119\n",
      "Iteration 520/540; Loss: 0.7635117769241333\n",
      "Iteration 540/540; Loss: 0.7542673349380493\n",
      "Checkpoint saved at models-test/epoch-1_iter-540_checkpoint.pth after 540 iterations of epoch 1\n",
      "[epoch: 1]\n",
      " - train loss: 0.7542673349380493, train accuracy: 0.6171151620370371\n",
      " - valid loss: 0.6616601943969727, valid accuracy: 0.6308449074074074\n",
      "- New best model saves with accuracy 0.6308%\n",
      "Total time for 2 epochs of 540 iterations: 11012.9586 seconds\n"
     ]
    }
   ],
   "source": [
    "# CREATING AND TRAINING THE ViT MODEL\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=len(dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "save_dir = 'models-test'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_every = 60\n",
    "print_every = 20\n",
    "\n",
    "initial = time.time()\n",
    "\n",
    "train_model(epochs, model, criterion, optimizer, train_loader, valid_loader, device, save_dir, save_every, print_every)\n",
    "\n",
    "final = time.time()\n",
    "\n",
    "print(f'Total time for {epochs} epochs of {len(train_loader)} iterations: {final - initial:.4f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds301",
   "language": "python",
   "name": "ds301"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
